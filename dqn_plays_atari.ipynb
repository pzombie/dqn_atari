{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import gym\n",
    "from memory import replay_buffer\n",
    "from DQN import DDQN\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image):\n",
    "    # Pretty crude, truncating, but fast, gray-scale preprocessing.\n",
    "    # Something better with a decent time-trade-off would be welcome\n",
    "    return (  image[::2, ::2, 0]//3\n",
    "            + image[::2, ::2, 1]//3\n",
    "            + image[::2, ::2, 2]//3).astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stats:\n",
    "    \"\"\"Store some statistics as the agent learns\"\"\"\n",
    "    def __init__(self):\n",
    "        self.alive_frames         = 0\n",
    "        self.total_reward         = 0\n",
    "        self.game_number          = 0\n",
    "        self.sum_predicted_reward = 0\n",
    "        self.policy_actions_taken = [0 for _ in range(NUM_ACTIONS)]\n",
    "        self.results              = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Atari emulator fired up\n",
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "env.reset()\n",
    "new_episode = True\n",
    "\n",
    "NUM_ACTIONS = env.action_space.n\n",
    "NUM_FRAMES  = 4 # Number of frames to stack for each state\n",
    "ATARI_SHAPE = (105, 80, NUM_FRAMES)\n",
    "\n",
    "# Define the DQN model\n",
    "dqn         = DDQN(NUM_ACTIONS, ATARI_SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "NUM_ITERATIONS         = 10000000 # How many frames the agent will experience\n",
    "MIN_OBSERVATION        = 50000    # Preload the experience replay buffer with some observations\n",
    "BUFFER_SIZE            = 1000000  # Experience replay buffer size\n",
    "MINIBATCH_SIZE         = 32       # Mnih et al. uses 32. Some other implementations experiment with larger batches\n",
    "\n",
    "EPSILON_DECAY          = 500000   # Define annealing schedule\n",
    "FINAL_EPSILON          = 0.01\n",
    "INTERMEDIATE_EPSILON   = 0.1\n",
    "INITIAL_EPSILON        = 1.0\n",
    "\n",
    "DECAY_RATE             = 0.99     # Discounted reward factor\n",
    "TARGET_NETWORK_PERIOD  = 10000    # How often to clone the Q network for predicting actions\n",
    "SAVE_PERIOD            = 250000   # Save the network weights periodically\n",
    "REPORT_EPISODE_PERIOD  = 100      # How many episodes before reporting stats\n",
    "\n",
    "NUM_RANDOM_ACTIONS_MAX = 7        # Maximum number of random actions to take after launching the ball\n",
    "FIRE_ACTION            = 1        # We're gonna have to fire to start each episode moving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare to begin learning process\n",
    "prev_time              = 0\n",
    "prev_iteration         = 0\n",
    "iteration_             = 0\n",
    "epsilon                = INITIAL_EPSILON\n",
    "t0                     = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initalise experience replay memory\n",
    "experience_buffer  = replay_buffer(BUFFER_SIZE)\n",
    "\n",
    "# Initalise statistics container\n",
    "learning_stats = stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Well, here we go...\n",
    "for iteration_ in range(iteration_, NUM_ITERATIONS + iteration_):\n",
    "    \n",
    "    #------------------------------------------------\n",
    "    #\n",
    "    #         Initialise Atari to random state\n",
    "    #\n",
    "    #------------------------------------------------\n",
    "    if new_episode:\n",
    "        new_episode = False\n",
    "        done = False\n",
    "        \n",
    "        # First of all, launch the ball as the first action of the episode\n",
    "        image_, reward_, done_, _ = env.step(FIRE_ACTION)\n",
    "        \n",
    "        # Then do a random number of random actions\n",
    "        num_random_actions = np.random.randint(4, NUM_RANDOM_ACTIONS_MAX)\n",
    "        for ii in range(num_random_actions):\n",
    "            action = np.random.randint(NUM_ACTIONS)\n",
    "\n",
    "            image_, reward_, done_, info = env.step(action)\n",
    "            \n",
    "            # The last NUM_FRAMES observations can go into the experience buffer\n",
    "            if ii >= num_random_actions - NUM_FRAMES:\n",
    "                experience_buffer.append(preprocess(image_), action, reward_, done_)\n",
    "            del image_\n",
    "            done = done | done_\n",
    "            \n",
    "            # Count loss of life as terminal (please agent, learn to not die)\n",
    "            if info['ale.lives'] != 5:\n",
    "                done = True   \n",
    "    else:\n",
    "        \n",
    "        #------------------------------------------------\n",
    "        #\n",
    "        #              Agent plays Atari\n",
    "        #\n",
    "        #------------------------------------------------\n",
    "        \n",
    "        # Recall latest state from observations \n",
    "        initial_state = experience_buffer.get_last_state()\n",
    "        \n",
    "        # Start off by playing randomly\n",
    "        if iteration_ < MIN_OBSERVATION:\n",
    "            action = np.random.randint(NUM_ACTIONS)\n",
    "        else:\n",
    "            if random.random() < epsilon:\n",
    "                # Occasionally explore with a random action\n",
    "                action = np.random.randint(NUM_ACTIONS)\n",
    "            else:\n",
    "                # Play from policy\n",
    "                q_predict = dqn.model.predict([initial_state.reshape((1,) + ATARI_SHAPE)] + [np.ones((1, NUM_ACTIONS))], \n",
    "                                              batch_size = 1)\n",
    "                \n",
    "                action = np.argmax(q_predict)\n",
    "                \n",
    "                # Save some stats for later\n",
    "                learning_stats.sum_predicted_reward += np.max(q_predict)\n",
    "                learning_stats.policy_actions_taken[action] += 1  \n",
    "                \n",
    "        # Take action, observe image, are we done?\n",
    "        image_, reward, done, info = env.step(action)\n",
    "\n",
    "        # Save experience to buffer\n",
    "        experience_buffer.append(preprocess(image_), action, reward, done)\n",
    "        del image_\n",
    "\n",
    "        # Count loss of life as terminal (please agent, learn to not die)\n",
    "        if info['ale.lives'] != 5:\n",
    "            done = True\n",
    "\n",
    "        # Recall latest state from observations    \n",
    "        state = experience_buffer.get_last_state()\n",
    "\n",
    "        #------------------------------------------------\n",
    "        #\n",
    "        #  Agent recalls previous experiences and learns\n",
    "        #\n",
    "        #------------------------------------------------\n",
    "\n",
    "        if iteration_ > MIN_OBSERVATION:\n",
    "            \n",
    "            indexes = experience_buffer.get_batch_indexes(MINIBATCH_SIZE)       \n",
    "            \n",
    "            s_now, a_batch, r_batch, d_batch, s_next = experience_buffer.get_batch(indexes)      \n",
    "            \n",
    "            # For the next state, what does the target model currently predict as the maximum discounted reward \n",
    "            target_q_values_next = dqn.target_model.predict([s_next] + [np.ones((MINIBATCH_SIZE, NUM_ACTIONS))], \n",
    "                                                            batch_size = MINIBATCH_SIZE)\n",
    "\n",
    "            targets = np.zeros((MINIBATCH_SIZE, NUM_ACTIONS))\n",
    "            for ii in range(MINIBATCH_SIZE):\n",
    "                targets[ii, a_batch[ii]] = r_batch[ii] # for the observed action, set reward\n",
    "                if d_batch[ii] == False:\n",
    "                    targets[ii, a_batch[ii]] += DECAY_RATE * np.max(target_q_values_next[ii, :])\n",
    "\n",
    "            action_mask = np.zeros((MINIBATCH_SIZE, NUM_ACTIONS))\n",
    "            action_mask[np.arange(MINIBATCH_SIZE), a_batch] = 1.0\n",
    "\n",
    "            dqn.model.train_on_batch([s_now] + [action_mask], targets.astype('float32'))\n",
    "            \n",
    "        # Ocassionally we'll copy the weights from the backprop'd model to the target model\n",
    "        # used for predicting the discounted future reward for an action\n",
    "        if iteration_ % TARGET_NETWORK_PERIOD == TARGET_NETWORK_PERIOD-1:\n",
    "            model_weights = dqn.model.get_weights()\n",
    "            dqn.target_model.set_weights(model_weights)\n",
    "            \n",
    "        #------------------------------------------------\n",
    "        #\n",
    "        # Record some more stats and reset the game once done\n",
    "        #\n",
    "        #------------------------------------------------\n",
    "\n",
    "        learning_stats.alive_frames += 1\n",
    "        learning_stats.total_reward += reward\n",
    "    \n",
    "    if done:\n",
    "        \n",
    "        learning_stats.results.append((learning_stats.total_reward, \n",
    "                                       learning_stats.sum_predicted_reward, \n",
    "                                       learning_stats.alive_frames, \n",
    "                                       epsilon))\n",
    "        \n",
    "        if learning_stats.game_number % REPORT_EPISODE_PERIOD == REPORT_EPISODE_PERIOD - 1:\n",
    "            \n",
    "            result = np.mean(np.array(learning_stats.results[-REPORT_EPISODE_PERIOD:]), axis = 0)\n",
    "            print('Played 100 episodes...')\n",
    "            print('Iteration', iteration_)\n",
    "            print('Total time', int(time.time() - t0))\n",
    "            print('FPS',  (iteration_ - prev_iteration)/(time.time() - prev_time))\n",
    "            print('Average reward', result[0])\n",
    "            print('Average Q predicted', result[1]/result[2])\n",
    "            print('Average alive frames', result[2])\n",
    "            print('Epsilon', result[3])\n",
    "            print('Policy actions', learning_stats.policy_actions_taken)\n",
    "            print()\n",
    "            prev_time = time.time()\n",
    "            prev_iteration = iteration_\n",
    "            learning_stats.policy_actions_taken = [0 for _ in range(NUM_ACTIONS)]\n",
    "        \n",
    "        # Reset for new episode\n",
    "        env.reset()\n",
    "        new_episode = True\n",
    "        learning_stats.alive_frames = 0\n",
    "        learning_stats.total_reward = 0\n",
    "        learning_stats.game_number += 1\n",
    "        learning_stats.sum_predicted_reward = 0\n",
    "        \n",
    "    # Annealing schedule    \n",
    "    if epsilon > INTERMEDIATE_EPSILON:\n",
    "        epsilon -= (INITIAL_EPSILON-INTERMEDIATE_EPSILON)/EPSILON_DECAY  \n",
    "    elif epsilon > FINAL_EPSILON:\n",
    "        epsilon -= (INTERMEDIATE_EPSILON-FINAL_EPSILON)/EPSILON_DECAY  \n",
    "      \n",
    "    # Save model every so often so we can interrupt learning and do warm restarts\n",
    "    if iteration_ % SAVE_PERIOD == SAVE_PERIOD - 1:\n",
    "        DQN.model.save('saved_' + str(iteration_+1) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
