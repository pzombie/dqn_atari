{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pzombie\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import gym\n",
    "from memory import replay_buffer\n",
    "from DQN import DDQN, huber_loss\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "import imageio\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports specifically so we can render outputs in Jupyter.\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from IPython.display import display\n",
    "\n",
    "def display_frames_as_gif(frames):\n",
    "    \"\"\"\n",
    "    Displays a list of frames as a gif, with controls\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(4*frames[0].shape[1] / 72.0, 4*frames[0].shape[0] / 72.0), dpi = 72)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval = 50)\n",
    "    display(display_animation(anim, default_mode = 'loop'))\n",
    "    \n",
    "    return anim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent():\n",
    "    #------------------------------------------------\n",
    "    #\n",
    "    #  Test agent by playing Atari from policy\n",
    "    #\n",
    "    #------------------------------------------------\n",
    "\n",
    "    # restart environment\n",
    "    frame_buffer = []\n",
    "    state_buffer = []\n",
    "\n",
    "    env.reset()\n",
    "    num_no_op_actions = np.random.randint(4, NUM_NO_OP_ACTIONS_MAX)\n",
    "    reward = 0\n",
    "    done = False\n",
    "\n",
    "    # release the ball!\n",
    "    image_, reward_, done_, _ = env.step(FIRE_ACTION)\n",
    "    frame_buffer.append(image_)\n",
    "    prev_lives = 5\n",
    "    alive_frames = 0     \n",
    "    actions = []\n",
    "    prev_livess = []\n",
    "    lives = []\n",
    "    # take a number of no-op actions to randomise start\n",
    "    for ii in range(num_no_op_actions):\n",
    "        del image_\n",
    "\n",
    "        image_, reward_, done_, info = env.step(NO_OP_ACTION)\n",
    "\n",
    "\n",
    "        frame_buffer.append(image_)\n",
    "        state_buffer.append(preprocess(image_))\n",
    "\n",
    "        reward += reward_\n",
    "        done = done | done_\n",
    "\n",
    "        # count loss of life as terminal (agent must learn to not die)\n",
    "        if info['ale.lives'] != prev_lives:\n",
    "            image_, reward_, done_, _ = env.step(FIRE_ACTION)\n",
    "            prev_lives = info['ale.lives']\n",
    "    while not done_:      \n",
    "        q_predict = dqn.model.predict([np.dstack(state_buffer[-4:][::1]).reshape((1,) + ATARI_SHAPE) ]\n",
    "                                      + [np.ones((1, NUM_ACTIONS))], \n",
    "                                      batch_size = 1)\n",
    "        action = np.argmax(q_predict)\n",
    "        if random.random() < 0.05:\n",
    "            action = np.random.randint(NUM_ACTIONS)\n",
    "        del image_\n",
    "        image_, reward_, done_, info = env.step(action)\n",
    "        reward += reward_\n",
    "        frame_buffer.append(image_.copy())  \n",
    "        state_buffer.append(preprocess(image_))\n",
    "        alive_frames += 1\n",
    "        actions.append(action)\n",
    "        lives.append(info['ale.lives'])\n",
    "        prev_livess.append(prev_lives)\n",
    "        if info['ale.lives'] != prev_lives:\n",
    "            image_, reward_, done_, _ = env.step(FIRE_ACTION)\n",
    "            prev_lives = info['ale.lives']\n",
    "    \n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image):\n",
    "    # Pretty crude, truncating, but fast, gray-scale preprocessing.\n",
    "    # Something better with a decent time-trade-off would be welcome\n",
    "    return (  image[::2, ::2, 0]//3\n",
    "            + image[::2, ::2, 1]//3\n",
    "            + image[::2, ::2, 2]//3).astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stats:\n",
    "    \"\"\"Store some statistics as the agent learns\"\"\"\n",
    "    def __init__(self):\n",
    "        self.alive_frames         = 0\n",
    "        self.total_reward         = 0\n",
    "        self.game_number          = 0\n",
    "        self.sum_predicted_reward = 0\n",
    "        self.policy_actions_taken = [0 for _ in range(NUM_ACTIONS)]\n",
    "        self.results              = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Atari emulator fired up\n",
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "env.reset()\n",
    "new_episode = True\n",
    "\n",
    "NUM_ACTIONS = env.action_space.n\n",
    "NUM_FRAMES  = 4 # Number of frames to stack for each state\n",
    "ATARI_SHAPE = (105, 80, NUM_FRAMES)\n",
    "\n",
    "# Define the DQN model\n",
    "dqn         = DDQN(NUM_ACTIONS, ATARI_SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "NUM_ITERATIONS         = 50000000 # How many frames the agent will experience\n",
    "MIN_OBSERVATION        = 50000    # Preload the experience replay buffer with some observations\n",
    "BUFFER_SIZE            = 1000000  # Experience replay buffer size\n",
    "MINIBATCH_SIZE         = 32       # Mnih et al. uses 32. Some other implementations experiment with larger batches\n",
    "\n",
    "EPSILON_DECAY          = 1000000  # Define annealing schedule (how many iterations to get to intermediate, then final eps)\n",
    "FINAL_EPSILON          = 0.1\n",
    "INTERMEDIATE_EPSILON   = 0.1\n",
    "INITIAL_EPSILON        = 1.0\n",
    "\n",
    "DECAY_RATE             = 0.99     # Discounted reward factor\n",
    "TARGET_NETWORK_PERIOD  = 10000    # How often to clone the Q network for predicting actions\n",
    "SAVE_PERIOD            = 250000   # Save the network weights periodically\n",
    "REPORT_EPISODE_PERIOD  = 100      # How many episodes before reporting stats\n",
    "\n",
    "NUM_NO_OP_ACTIONS_MAX  = 7        # Maximum number of random actions to take after launching the ball\n",
    "FIRE_ACTION            = 1        # We're gonna have to fire to start each episode moving\n",
    "NO_OP_ACTION           = 0        # Do nothing (no operation) action\n",
    "\n",
    "UPDATE_FREQUENCY       = 4        # Number of actions agent takes between SGD updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare to begin learning process\n",
    "prev_time              = 0\n",
    "prev_iteration         = 0\n",
    "iteration_             = 0\n",
    "epsilon                = INITIAL_EPSILON\n",
    "t0                     = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initalise experience replay memory\n",
    "experience_buffer  = replay_buffer(BUFFER_SIZE)\n",
    "\n",
    "# Initalise statistics container\n",
    "learning_stats = stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dqn.model        = load_model('saved_2250000.h5', custom_objects={'huber_loss': huber_loss})\n",
    "# dqn.target_model = load_model('saved_2250000.h5', custom_objects={'huber_loss': huber_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dqn.model.compile(optimizer=Adam(lr=0.000025), loss=huber_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_episode = True\n",
    "output_filename = 'results.txt'\n",
    "results_file = open(output_filename, 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_file.write(\"Iteration, time, FPS, Average Reward, Average Q predicted, Average alive frames, epsilon, test_reward\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_lives = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Played 100 episodes...\n",
      "Iteration 2470\n",
      "Total time 6\n",
      "FPS 1.6156518478712096e-06\n",
      "Average reward 0.19\n",
      "Average Q predicted 0.0\n",
      "Average alive frames 24.7\n",
      "Epsilon 0.9988014610000395\n",
      "Policy actions [0, 0, 0, 0]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'num_random_actions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-695037c780b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    141\u001b[0m             \u001b[0mtest_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mii\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m                 \u001b[0mtest_rewards\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_agent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m             \u001b[0mtest_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_rewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m             \u001b[0mtest_min\u001b[0m    \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_rewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-0a736532fdbf>\u001b[0m in \u001b[0;36mtest_agent\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mlives\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;31m# take a number of no-op actions to randomise start\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mii\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_random_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mimage_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_random_actions' is not defined"
     ]
    }
   ],
   "source": [
    "# Well, here we go...\n",
    "while iteration_ < NUM_ITERATIONS:\n",
    "    \n",
    "    #------------------------------------------------\n",
    "    #\n",
    "    #         Initialise Atari to random state\n",
    "    #\n",
    "    #------------------------------------------------\n",
    "    if new_episode:\n",
    "        new_episode = False\n",
    "        done = False\n",
    "        \n",
    "        # First of all, launch the ball as the first action of the episode\n",
    "        image_, reward_, done, info = env.step(FIRE_ACTION)\n",
    "        \n",
    "        # Then do a random number of no_op actions\n",
    "        num_no_op_actions = np.random.randint(4, NUM_NO_OP_ACTIONS_MAX)\n",
    "        for ii in range(num_no_op_actions):\n",
    "            image_, reward_, done, info = env.step(NO_OP_ACTION)\n",
    "            \n",
    "            # Count loss of life as terminal (please agent, learn to not die)\n",
    "            # But don't reset until all lives are lost\n",
    "            if info['ale.lives'] != prev_lives:\n",
    "                done = True   \n",
    "            \n",
    "            # The last NUM_FRAMES observations can go into the experience buffer\n",
    "            if ii >= num_no_op_actions - NUM_FRAMES:\n",
    "                experience_buffer.append(preprocess(image_), NO_OP_ACTION, reward_, done)\n",
    "            del image_\n",
    "        \n",
    "            \n",
    "    else:\n",
    "        \n",
    "        #------------------------------------------------\n",
    "        #\n",
    "        #              Agent plays Atari\n",
    "        #\n",
    "        #------------------------------------------------\n",
    "        \n",
    "        # Recall latest state from observations \n",
    "        initial_state = experience_buffer.get_last_state()\n",
    "        action = NO_OP_ACTION # default action - will get overwritten in a moment\n",
    "        if iteration_ < MIN_OBSERVATION:\n",
    "            # Start off by playing randomly\n",
    "            action = np.random.randint(NUM_ACTIONS)\n",
    "        else:\n",
    "            if random.random() < epsilon:\n",
    "                # Occasionally explore with a random action\n",
    "                action = np.random.randint(NUM_ACTIONS)\n",
    "            else:\n",
    "                # Play from policy\n",
    "                q_predict = dqn.model.predict([initial_state.reshape((1,) + ATARI_SHAPE)] + [np.ones((1, NUM_ACTIONS))], \n",
    "                                              batch_size = 1)\n",
    "\n",
    "                action = np.argmax(q_predict)\n",
    "\n",
    "                # Save some stats for later\n",
    "                learning_stats.sum_predicted_reward += np.max(q_predict)\n",
    "                learning_stats.policy_actions_taken[action] += 1  \n",
    "\n",
    "        # Take action, observe image, are we done?\n",
    "        image_, reward_, done, info = env.step(action)\n",
    "        \n",
    "        # Count loss of life as terminal (please agent, learn to not die)\n",
    "        # But don't reset until all lives are lost\n",
    "        if info['ale.lives'] != prev_lives:\n",
    "            done = True    \n",
    "\n",
    "        # Save experience to buffer\n",
    "        experience_buffer.append(preprocess(image_), action, reward_, done)\n",
    "        del image_\n",
    "            \n",
    "        learning_stats.alive_frames += 1\n",
    "        learning_stats.total_reward += reward_   \n",
    "            \n",
    "        iteration_ += 1\n",
    "\n",
    "        #------------------------------------------------\n",
    "        #\n",
    "        #  Agent recalls previous experiences and learns\n",
    "        #\n",
    "        #------------------------------------------------\n",
    "\n",
    "        \n",
    "        if (iteration_ > MIN_OBSERVATION) and (iteration_ % UPDATE_FREQUENCY == 0):\n",
    "\n",
    "            s_now, a_batch, r_batch, d_batch, s_next = experience_buffer.get_batch(MINIBATCH_SIZE)      \n",
    "            \n",
    "            # For the next state, what does the target model currently predict as the maximum discounted reward \n",
    "            target_q_values_next = dqn.target_model.predict([s_next] + [np.ones((MINIBATCH_SIZE, NUM_ACTIONS))], \n",
    "                                                            batch_size = MINIBATCH_SIZE)\n",
    "            \n",
    "            # Use the main network to predict the action to take\n",
    "            q_values_next = dqn.model.predict([s_next] + [np.ones((MINIBATCH_SIZE, NUM_ACTIONS))], \n",
    "                                                            batch_size = MINIBATCH_SIZE)        \n",
    "            actions_batch = np.argmax(q_values_next, axis = 1)\n",
    "            \n",
    "            # Use the target network to predict the Q-value of that action\n",
    "            target_q_values = target_q_values_next[np.arange(MINIBATCH_SIZE), actions_batch]\n",
    "            \n",
    "            targets = np.zeros((MINIBATCH_SIZE, NUM_ACTIONS))\n",
    "            for ii in range(MINIBATCH_SIZE):\n",
    "                targets[ii, a_batch[ii]] = r_batch[ii] # for the observed action, set reward\n",
    "                if d_batch[ii] == False:\n",
    "                    targets[ii, a_batch[ii]] += DECAY_RATE * target_q_values[ii]\n",
    "\n",
    "            action_mask = np.zeros((MINIBATCH_SIZE, NUM_ACTIONS))\n",
    "            action_mask[np.arange(MINIBATCH_SIZE), a_batch] = 1.0\n",
    "\n",
    "            dqn.model.train_on_batch([s_now] + [action_mask], targets.astype('float32'))\n",
    "            \n",
    "        # Ocassionally we'll copy the weights from the backprop'd model to the target model\n",
    "        # used for predicting the discounted future reward for an action\n",
    "        if iteration_ % TARGET_NETWORK_PERIOD == TARGET_NETWORK_PERIOD-1:\n",
    "            model_weights = dqn.model.get_weights()\n",
    "            dqn.target_model.set_weights(model_weights)\n",
    "            \n",
    "            print('Cloning model @ iteration', iteration_)\n",
    "            print()\n",
    "    \n",
    "    if done:\n",
    "        \n",
    "        learning_stats.results.append((learning_stats.total_reward, \n",
    "                                       learning_stats.sum_predicted_reward, \n",
    "                                       learning_stats.alive_frames, \n",
    "                                       epsilon))\n",
    "        \n",
    "        if learning_stats.game_number % REPORT_EPISODE_PERIOD == REPORT_EPISODE_PERIOD - 1:\n",
    "            \n",
    "            result = np.mean(np.array(learning_stats.results[-REPORT_EPISODE_PERIOD:]), axis = 0)\n",
    "            print('Played 100 episodes...')\n",
    "            print('Iteration', iteration_)\n",
    "            print('Total time', int(time.time() - t0))\n",
    "            print('FPS',  (iteration_ - prev_iteration)/(time.time() - prev_time))\n",
    "            print('Average reward', result[0])\n",
    "            print('Average Q predicted', result[1]/result[2])\n",
    "            print('Average alive frames', result[2])\n",
    "            print('Epsilon', result[3])\n",
    "            print('Policy actions', learning_stats.policy_actions_taken)\n",
    "            \n",
    "            test_rewards = []\n",
    "            for ii in range(10):\n",
    "                test_rewards.append(test_agent())\n",
    "            test_reward = np.mean(test_rewards)\n",
    "            test_min    = np.min(test_rewards)\n",
    "            test_max    = np.max(test_rewards)\n",
    "            print('Test Reward', test_reward) \n",
    "            print('Test Reward [min-max] [', test_min, test_max, ']') \n",
    "            \n",
    "            print()\n",
    "            prev_time = time.time()\n",
    "            prev_iteration = iteration_\n",
    "            learning_stats.policy_actions_taken = [0 for _ in range(NUM_ACTIONS)]\n",
    "            \n",
    "            results_file = open(output_filename, 'a')\n",
    "            \n",
    "            results_file.write(\"%f, %f, %f, %f, %f, %f, %f, %f\\n\" % (iteration_, \n",
    "                                                 int(time.time() - t0), \n",
    "                                                 (iteration_ - prev_iteration)/(time.time() - prev_time),\n",
    "                                                 result[0],\n",
    "                                                 result[1]/result[2],\n",
    "                                                 result[2],\n",
    "                                                 result[3],\n",
    "                                                 test_reward))\n",
    "        \n",
    "        # Reset for new episode\n",
    "        if info['ale.lives'] == 0:\n",
    "            env.reset()\n",
    "        new_episode = True\n",
    "        learning_stats.alive_frames = 0\n",
    "        learning_stats.total_reward = 0\n",
    "        learning_stats.game_number += 1\n",
    "        learning_stats.sum_predicted_reward = 0\n",
    "        \n",
    "        # Kick off the next episode if necessary (FIRE action launches the ball)\n",
    "        if prev_lives != info['ale.lives']:\n",
    "            _, _, _, info = env.step(FIRE_ACTION)\n",
    "            prev_lives = info['ale.lives']\n",
    "        \n",
    "    # Annealing schedule    \n",
    "    if epsilon > INTERMEDIATE_EPSILON:\n",
    "        epsilon -= (INITIAL_EPSILON-INTERMEDIATE_EPSILON)/EPSILON_DECAY  \n",
    "    elif epsilon > FINAL_EPSILON:\n",
    "        epsilon -= (INTERMEDIATE_EPSILON-FINAL_EPSILON)/EPSILON_DECAY  \n",
    "      \n",
    "    # Save model every so often so we can interrupt learning and do warm restarts\n",
    "    if iteration_ % SAVE_PERIOD == SAVE_PERIOD - 1:\n",
    "        dqn.model.save('saved_' + str(iteration_+1) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------\n",
    "#\n",
    "#  Test agent by playing Atari from policy\n",
    "#\n",
    "#------------------------------------------------\n",
    "\n",
    "# restart environment\n",
    "frame_buffer = []\n",
    "state_buffer = []\n",
    "\n",
    "env.reset()\n",
    "num_no_op_actions = np.random.randint(4, NUM_NO_OP_ACTIONS_MAX)\n",
    "reward = 0\n",
    "done = False\n",
    "\n",
    "# release the ball!\n",
    "image_, reward_, done_, _ = env.step(FIRE_ACTION)\n",
    "frame_buffer.append(image_)\n",
    "prev_lives = 5\n",
    "alive_frames = 0     \n",
    "actions = []\n",
    "prev_livess = []\n",
    "lives = []\n",
    "# take a number of no-op actions to randomise start\n",
    "for ii in range(num_no_op_actions):\n",
    "    del image_\n",
    "    \n",
    "    image_, reward_, done_, info = env.step(NO_OP_ACTION)\n",
    "\n",
    "\n",
    "    frame_buffer.append(image_)\n",
    "    state_buffer.append(preprocess(image_))\n",
    "\n",
    "    reward += reward_\n",
    "    done = done | done_\n",
    "\n",
    "    # count loss of life as terminal (agent must learn to not die)\n",
    "    if info['ale.lives'] != 5:\n",
    "        done = True\n",
    "while not done_:      \n",
    "    q_predict = dqn.model.predict([np.dstack(state_buffer[-4:][::1]).reshape((1,) + ATARI_SHAPE) ]\n",
    "                                  + [np.ones((1, NUM_ACTIONS))], \n",
    "                                  batch_size = 1)\n",
    "    action = np.argmax(q_predict)\n",
    "    if random.random() < 0.05:\n",
    "        action = np.random.randint(NUM_ACTIONS)\n",
    "    del image_\n",
    "    image_, reward_, done_, info = env.step(action)\n",
    "    frame_buffer.append(image_.copy())  \n",
    "    state_buffer.append(preprocess(image_))\n",
    "    alive_frames += 1\n",
    "    actions.append(action)\n",
    "    lives.append(info['ale.lives'])\n",
    "    prev_livess.append(prev_lives)\n",
    "    if info['ale.lives'] != prev_lives:\n",
    "        print('Launching Ball')\n",
    "        image_, reward_, done_, _ = env.step(FIRE_ACTION)\n",
    "        prev_lives = info['ale.lives']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lives)\n",
    "plt.plot(prev_livess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show video of agent playing Atari\n",
    "anim = display_frames_as_gif(frame_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save video as an animated gif\n",
    "plt.rcParams[\"animation.convert_path\"] = 'C:\\Program Files\\ImageMagick-7.0.7-Q16\\magick.exe'\n",
    "writer = animation.ImageMagickFileWriter(fps = 30)\n",
    "anim.save(filename = \"breakout_4.gif\", writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
